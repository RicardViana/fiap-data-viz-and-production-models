{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d6ca84",
   "metadata": {},
   "source": [
    "#### **An√°lise de neg√≥cios**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580d69f",
   "metadata": {},
   "source": [
    "#### **Observa√ß√µes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59115f",
   "metadata": {},
   "source": [
    "#### **Conte√∫do - Bases e Notebook da aula**\n",
    "\n",
    "Github:  \n",
    "\n",
    "https://github.com/FIAP/Pos_Tech_DTAT/tree/Analise-de-Negocios/Analise%20de%20Negocios  \n",
    "\n",
    "S√©rie Hist√≥rica de Pre√ßos de Combust√≠veis e de GLP:  \n",
    "\n",
    "https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/serie-historica-de-precos-de-combustiveis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770d68e",
   "metadata": {},
   "source": [
    "#### **Importa√ß√£o de pacotes e bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e53e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar biblioteca completa\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import zipfile  \n",
    "import io       \n",
    "import time\n",
    "import concurrent.futures\n",
    "import psycopg2 \n",
    "import csv\n",
    "\n",
    "# Importar algo especifico de uma biblioteca\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21512f93",
   "metadata": {},
   "source": [
    "#### **Fun√ß√µes (def)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrar_links(soup):\n",
    "    \n",
    "    \"\"\"\n",
    "    Encontra dinamicamente os links de download na se√ß√£o \n",
    "    'Combust√≠veis automotivos'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encontra o cabe√ßalho <h3> que cont√©m o texto \"Combust√≠veis automotivos\"\n",
    "    heading = soup.find(lambda tag: tag.name == 'h3' and 'Combust√≠veis automotivos' in tag.get_text())\n",
    "    \n",
    "    links_para_baixar = []\n",
    "    \n",
    "    if not heading:\n",
    "        print(\"Erro: N√£o foi poss√≠vel encontrar a se√ß√£o 'Combust√≠veis automotivos' no HTML da p√°gina\")\n",
    "        return links_para_baixar\n",
    "\n",
    "    # A lista <ul> com os links √© o pr√≥ximo \"irm√£o\" (sibling) da tag <h3>\n",
    "    ul_tag = heading.find_next_sibling('ul')\n",
    "    \n",
    "    if not ul_tag:\n",
    "        print(\"Erro: N√£o foi poss√≠vel encontrar a lista <ul> ap√≥s o cabe√ßalho\")\n",
    "        return links_para_baixar\n",
    "\n",
    "    # Encontra todas as tags <a> (links) dentro desta lista <ul>\n",
    "    a_tags = ul_tag.find_all('a')\n",
    "    \n",
    "    for a_tag in a_tags:\n",
    "        url = a_tag.get('href')\n",
    "        if url:\n",
    "            links_para_baixar.append(url)\n",
    "            \n",
    "    return links_para_baixar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa7c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_arquivo(url, pasta_destino, max_retries, retry_delay):\n",
    "\n",
    "    \"\"\"\n",
    "    Baixa um arquivo com retentativas. Se for .zip, extrai. Se .csv, salva.\n",
    "    RETORNA uma string de status em vez de imprimir.\n",
    "    \"\"\"\n",
    "\n",
    "    nome_arquivo = os.path.basename(url)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            \n",
    "            if nome_arquivo.endswith('.zip'):\n",
    "                response = requests.get(url) \n",
    "                response.raise_for_status()\n",
    "                \n",
    "                with zipfile.ZipFile(io.BytesIO(response.content)) as zf:\n",
    "                    zf.extractall(pasta_destino)\n",
    "                    nomes_extraidos = zf.namelist()\n",
    "                \n",
    "                return f\"[EXTRA√çDO] {nome_arquivo} -> {', '.join(nomes_extraidos)}\"\n",
    "\n",
    "            else:\n",
    "                caminho_local = os.path.join(pasta_destino, nome_arquivo)\n",
    "                \n",
    "                with requests.get(url, stream=True) as r:\n",
    "                    r.raise_for_status()\n",
    "                    with open(caminho_local, 'wb') as f:\n",
    "                        for chunk in r.iter_content(chunk_size=8192): \n",
    "                            f.write(chunk)\n",
    "                            \n",
    "                return f\"[SALVO] {nome_arquivo}\"\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt + 1 < max_retries:\n",
    "                time.sleep(retry_delay)\n",
    "            else: \n",
    "                return f\"[FALHA-REDE] {nome_arquivo} ap√≥s {max_retries} tentativas. Erro: {e}\"\n",
    "        \n",
    "        except zipfile.BadZipFile:\n",
    "            return f\"[FALHA-ZIP] {nome_arquivo} est√° corrompido ou n√£o √© .zip.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"[FALHA-INESPERADA] {nome_arquivo}. Erro: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31131aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar a conex√£o ao banco de dados\n",
    "def test_connection(engine):\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            \n",
    "            # Testar a vers√£o do PostgreSQL\n",
    "            result = connection.execute(text(\"SELECT version();\"))\n",
    "            versao = result.fetchone()\n",
    "            print(\"‚úÖ Conectado com sucesso:\", versao[0])\n",
    "\n",
    "            # Listar as tabelas no schema p√∫blico\n",
    "            result = connection.execute(text(\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = 'anp';\n",
    "            \"\"\"))\n",
    "            tabelas = result.fetchall()\n",
    "            print(\"üìÑ Tabelas no banco:\")\n",
    "            for tabela in tabelas:\n",
    "                print(\"-\", tabela[0])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Erro ao executar comandos:\", e)\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06d05d",
   "metadata": {},
   "source": [
    "#### **Credenciais**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a0ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Credenciais do PostgreSQL\n",
    "usuario_pg = os.getenv(\"POSTGRES_USER\")\n",
    "senha_pg = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "host_pg = os.getenv(\"POSTGRES_HOST\")\n",
    "porta_pg = os.getenv(\"POSTGRES_PORT\")\n",
    "banco_pg = os.getenv(\"POSTGRES_DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ad762",
   "metadata": {},
   "source": [
    "#### **Variaveis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N√∫mero m√°ximo de tentativas por arquivo\n",
    "max_retries = 5\n",
    "\n",
    "# Segundos de espera entre as tentativas\n",
    "retry_delay = 5  \n",
    "\n",
    "# Maximo de threads\n",
    "max_workers = 20\n",
    "\n",
    "# URL da p√°gina para extrair os links\n",
    "page_url = 'https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/serie-historica-de-precos-de-combustiveis'\n",
    "\n",
    "# Pasta onde os arquivos ser√£o baixados\n",
    "download_dir = 'arquivos_combustiveis_automotivos'\n",
    "\n",
    "# Validar download dos arquivos ANP\n",
    "baixar_arquivos_anp = 'n'\n",
    "\n",
    "# Nome da tabela no banco de dados \n",
    "nome_tabela = 'anp.preco_combustivel' \n",
    "\n",
    "# Chunksize para carga no banco de dados\n",
    "chunksize = 100000     \n",
    "\n",
    "# Validar carga no banco de dados\n",
    "carregar_tabela = 's' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d70f28a",
   "metadata": {},
   "source": [
    "#### **Aula 1 - Processos e formas de an√°lise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4ed4ab",
   "metadata": {},
   "source": [
    "#### **Aula 2 - Liga√ß√£o com bancos de dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737c07e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar engine com banco \n",
    "engine = create_engine(f\"postgresql+psycopg2://{usuario_pg}:{senha_pg}@{host_pg}:{porta_pg}/{banco_pg}\")\n",
    "\n",
    "# Testar a conex√£o\n",
    "test_connection(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598075b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar os arquivos da ANP\n",
    "\n",
    "if baixar_arquivos_anp.lower() == 'n':\n",
    "    print(f'Etapa de carregar os dados do Github para o PostgreSQL n√£o realizada pois a variavel baixar_arquivos_anp √© `n`')\n",
    "\n",
    "else:\n",
    "\n",
    "    # 1. Criar a pasta de download\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    # 2. Baixar o HTML da p√°gina da ANP\n",
    "    print(f\"Acessando a p√°gina: {page_url}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(page_url)\n",
    "        response.raise_for_status()\n",
    "        conteudo_html = response.text\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erro fatal ao acessar a p√°gina da ANP: {e}\")\n",
    "        print(\"Verifique sua conex√£o com a internet ou se a URL da ANP mudou\")\n",
    "        sys.exit(1) \n",
    "\n",
    "    # 3. Analisar (parse) o HTML\n",
    "    soup = BeautifulSoup(conteudo_html, 'html.parser')\n",
    "\n",
    "    # 4. Encontrar os links\n",
    "    links = encontrar_links(soup)\n",
    "\n",
    "    if not links:\n",
    "        print(\"Nenhum link encontrado para baixar\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Encontrados {len(links)} arquivos para baixar na se√ß√£o 'Combust√≠veis automotivos'\")\n",
    "        print()\n",
    "\n",
    "        # 5. Processar (baixar ou extrair) cada arquivo EM PARALELO\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            print(f\"Iniciando downloads com at√© {max_workers} threads em paralelo...\")\n",
    "            print(\"-\" * 70)\n",
    "            future_to_url = {}\n",
    "\n",
    "            for url in links:\n",
    "                future = executor.submit(processar_arquivo, \n",
    "                                        url, \n",
    "                                        download_dir, \n",
    "                                        max_retries, \n",
    "                                        retry_delay)\n",
    "                \n",
    "                future_to_url[future] = url\n",
    "\n",
    "            num_concluidos = 0\n",
    "\n",
    "            for future in concurrent.futures.as_completed(future_to_url):\n",
    "                num_concluidos += 1\n",
    "                url = future_to_url[future]\n",
    "                \n",
    "                try:\n",
    "                    status_message = future.result()\n",
    "                    print(f\"({num_concluidos}/{len(links)}) {status_message}\")\n",
    "                    \n",
    "                except Exception as exc:\n",
    "                    print(f\"({num_concluidos}/{len(links)}) [FALHA-GERAL] {url} gerou uma exce√ß√£o: {exc}\")\n",
    "                    \n",
    "        print(\"-\" * 70)\n",
    "        print()\n",
    "        print(\"Download e processamento de todos os arquivos conclu√≠do\")\n",
    "        print(f\"Os arquivos est√£o salvos em: {os.path.abspath(download_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ce369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados no banco de dados \n",
    "\n",
    "colunas_tabela_pg = [\n",
    "    'regiao', 'estado', 'municipio', 'revenda', 'cnpj', 'nome_rua', \n",
    "    'numero_rua', 'complemento', 'bairro', 'cep', 'produto', \n",
    "    'data_coleta', 'valor_venda', 'unidade_medida', 'bandeira'\n",
    "]\n",
    "\n",
    "colunas_sql_copy = ', '.join(f'\"{col}\"' for col in colunas_tabela_pg) \n",
    "\n",
    "mapeamento_colunas_csv_para_pg = {\n",
    "    'Regiao - Sigla': 'regiao', 'Estado - Sigla': 'estado', 'Municipio': 'municipio',\n",
    "    'Revenda': 'revenda', 'CNPJ da Revenda': 'cnpj', 'Nome da Rua': 'nome_rua',\n",
    "    'Numero Rua': 'numero_rua', 'Complemento': 'complemento', 'Bairro': 'bairro',\n",
    "    'Cep': 'cep', 'Produto': 'produto', 'Data da Coleta': 'data_coleta',\n",
    "    'Valor de Venda': 'valor_venda', 'Unidade de Medida': 'unidade_medida',\n",
    "    'Bandeira': 'bandeira'\n",
    "}\n",
    "\n",
    "conn_str_psycopg = f\"dbname='{banco_pg}' user='{usuario_pg}' password='{senha_pg}' host='{host_pg}' port='{porta_pg}'\"\n",
    "\n",
    "\n",
    "if carregar_tabela.lower() == 'n':\n",
    "    print(f'Etapa de carregar os dados para o PostgreSQL n√£o realizada pois a variavel carregar_tabela √© `n`')\n",
    "\n",
    "else:\n",
    "    print(f\"\\nIniciando carga de dados via COPY para a tabela '{nome_tabela}' a partir de '{download_dir}'\")\n",
    "    \n",
    "    conn_psycopg = None \n",
    "    cursor = None       \n",
    "    \n",
    "    try:\n",
    "        print(\"Conectando ao PostgreSQL via psycopg2...\")\n",
    "        conn_psycopg = psycopg2.connect(conn_str_psycopg)\n",
    "        conn_psycopg.autocommit = False \n",
    "        cursor = conn_psycopg.cursor()\n",
    "        print(\"‚úÖ Conectado com sucesso.\")\n",
    "\n",
    "        print(f\"\\nProcurando arquivos .csv em '{download_dir}'...\")\n",
    "        arquivos_csv = sorted([f for f in os.listdir(download_dir) if f.endswith('.csv')]) \n",
    "        \n",
    "        if not arquivos_csv:\n",
    "             print(\"Nenhum arquivo .csv encontrado na pasta. Carga n√£o realizada.\")\n",
    "        else:\n",
    "            print(f\"Encontrados {len(arquivos_csv)} arquivos .csv. Iniciando carga via COPY...\")\n",
    "            arquivos_processados = 0\n",
    "            arquivos_com_erro = 0\n",
    "\n",
    "            for nome_arquivo_csv in arquivos_csv:\n",
    "                caminho_completo = os.path.join(download_dir, nome_arquivo_csv)\n",
    "                print(f\"\\n--- Processando arquivo: {nome_arquivo_csv} ({arquivos_processados + arquivos_com_erro + 1}/{len(arquivos_csv)}) ---\")\n",
    "                \n",
    "                chunk_iterator = None\n",
    "                encodings_to_try = ['utf-8', 'latin-1']\n",
    "                \n",
    "                for encoding_attempt in encodings_to_try:\n",
    "                    try:\n",
    "                        print(f\"  Tentando ler com encoding: {encoding_attempt}...\")\n",
    "                        chunk_iterator = pd.read_csv(\n",
    "                            caminho_completo, \n",
    "                            chunksize=chunksize, \n",
    "                            low_memory=False, \n",
    "                            sep=';',              \n",
    "                            encoding=encoding_attempt, \n",
    "                            decimal=',',          \n",
    "                            parse_dates=['Data da Coleta'], \n",
    "                            dayfirst=True         \n",
    "                        )\n",
    "                        print(f\"  Sucesso ao ler com {encoding_attempt}.\")\n",
    "                        break \n",
    "                    \n",
    "                    except UnicodeDecodeError:\n",
    "                        print(f\"  Falha ao ler com {encoding_attempt}. Tentando pr√≥ximo...\")\n",
    "                        if encoding_attempt == encodings_to_try[-1]: \n",
    "                            raise \n",
    "                    \n",
    "                    except FileNotFoundError: \n",
    "                         raise\n",
    "                    \n",
    "                    except Exception as e_read: \n",
    "                        print(f\"  Erro inesperado ao tentar ler com {encoding_attempt}: {e_read}\")\n",
    "                        raise \n",
    "\n",
    "                if chunk_iterator is None:\n",
    "                     raise Exception(\"N√£o foi poss√≠vel iniciar a leitura do arquivo CSV\")\n",
    "\n",
    "                try:\n",
    "                    total_chunks = 0\n",
    "\n",
    "                    for i, chunk in enumerate(chunk_iterator):\n",
    "                        total_chunks = i + 1\n",
    "                        \n",
    "                        try:\n",
    "                            chunk_renamed = chunk.rename(columns=mapeamento_colunas_csv_para_pg)\n",
    "\n",
    "                        except Exception as e_rename:\n",
    "                            print(f\"    ERRO ao renomear colunas no chunk {total_chunks}: {e_rename}\")\n",
    "                            raise \n",
    "\n",
    "                        chunk_reordered = chunk_renamed.reindex(columns=colunas_tabela_pg)\n",
    "\n",
    "                        # 3. Preparar buffer\n",
    "                        buffer = io.StringIO()\n",
    "\n",
    "                        chunk_reordered.to_csv(buffer, index=False, header=False, sep=',', \n",
    "                                               na_rep='\\\\N', quoting=csv.QUOTE_MINIMAL, \n",
    "                                               date_format='%Y-%m-%d')\n",
    "                        \n",
    "                        buffer.seek(0) \n",
    "\n",
    "                        # 4. Comando COPY\n",
    "                        print(f\"Carregando chunk {total_chunks}...\")\n",
    "                        sql_copy_command = f\"\"\"COPY {nome_tabela} ({colunas_sql_copy}) FROM STDIN WITH (FORMAT CSV, HEADER FALSE, NULL '\\\\N', DELIMITER ',')\"\"\"\n",
    "                        cursor.copy_expert(sql_copy_command, buffer)\n",
    "                        \n",
    "                    print(f\"-> Arquivo {nome_arquivo_csv} carregado ({total_chunks} chunks)\")\n",
    "                    conn_psycopg.commit() \n",
    "                    arquivos_processados += 1\n",
    "\n",
    "                except pd.errors.EmptyDataError:\n",
    "                    print(f\"  AVISO: Arquivo {nome_arquivo_csv} est√° vazio ou tornou-se vazio ap√≥s leitura\")\n",
    "                    conn_psycopg.rollback() \n",
    "                    arquivos_com_erro += 1 \n",
    "\n",
    "                except FileNotFoundError: \n",
    "                    print(f\"‚ùå Erro: Arquivo {nome_arquivo_csv} n√£o encontrado durante processamento dos chunks\")\n",
    "                    conn_psycopg.rollback()\n",
    "                    arquivos_com_erro += 1\n",
    "\n",
    "                except Exception as e_file:\n",
    "                    print(f\"‚ùå Erro ao processar o arquivo {nome_arquivo_csv} (no chunk {total_chunks}): {e_file}\")\n",
    "                    print(f\"    Verifique o mapeamento, tipos de dados (especialmente datas e decimais).\")\n",
    "                    conn_psycopg.rollback() \n",
    "                    arquivos_com_erro += 1\n",
    "            \n",
    "            print(\"-\" * 70)\n",
    "            print(f\"\\n‚úÖ Carga via COPY conclu√≠da.\")\n",
    "            print(f\"  Arquivos processados com sucesso: {arquivos_processados}\")\n",
    "            print(f\"  Arquivos com erro/vazios: {arquivos_com_erro}\")\n",
    "\n",
    "    except psycopg2.Error as db_err:\n",
    "        print(f\"‚ùå Erro de conex√£o ou execu√ß√£o no PostgreSQL (psycopg2): {db_err}\")\n",
    "        if conn_psycopg:\n",
    "            conn_psycopg.rollback() \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro inesperado durante a carga dos dados (possivelmente ao iniciar leitura de {nome_arquivo_csv}): {e}\")\n",
    "        if conn_psycopg:\n",
    "            conn_psycopg.rollback()\n",
    "            \n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn_psycopg:\n",
    "            conn_psycopg.close()\n",
    "            print(\"\\nConex√£o psycopg2 fechada.\")\n",
    "            \n",
    "    print(\"\\nProcesso de carga finalizado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
