{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d6ca84",
   "metadata": {},
   "source": [
    "#### **Análise de negócios**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580d69f",
   "metadata": {},
   "source": [
    "#### **Observações**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59115f",
   "metadata": {},
   "source": [
    "#### **Conteúdo - Bases e Notebook da aula**\n",
    "\n",
    "Github:  \n",
    "\n",
    "https://github.com/FIAP/Pos_Tech_DTAT/tree/Analise-de-Negocios/Analise%20de%20Negocios  \n",
    "\n",
    "Série Histórica de Preços de Combustíveis e de GLP:  \n",
    "\n",
    "https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/serie-historica-de-precos-de-combustiveis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770d68e",
   "metadata": {},
   "source": [
    "#### **Importação de pacotes e bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7e53e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar biblioteca completa\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import zipfile  \n",
    "import io       \n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "# Importar algo especifico de uma biblioteca\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21512f93",
   "metadata": {},
   "source": [
    "#### **Funções (def)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrar_links(soup):\n",
    "    \n",
    "    \"\"\"\n",
    "    Encontra dinamicamente os links de download na seção \n",
    "    'Combustíveis automotivos'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encontra o cabeçalho <h3> que contém o texto \"Combustíveis automotivos\"\n",
    "    heading = soup.find(lambda tag: tag.name == 'h3' and 'Combustíveis automotivos' in tag.get_text())\n",
    "    \n",
    "    links_para_baixar = []\n",
    "    \n",
    "    if not heading:\n",
    "        print(\"Erro: Não foi possível encontrar a seção 'Combustíveis automotivos' no HTML da página\")\n",
    "        return links_para_baixar\n",
    "\n",
    "    # A lista <ul> com os links é o próximo \"irmão\" (sibling) da tag <h3>\n",
    "    ul_tag = heading.find_next_sibling('ul')\n",
    "    \n",
    "    if not ul_tag:\n",
    "        print(\"Erro: Não foi possível encontrar a lista <ul> após o cabeçalho\")\n",
    "        return links_para_baixar\n",
    "\n",
    "    # Encontra todas as tags <a> (links) dentro desta lista <ul>\n",
    "    a_tags = ul_tag.find_all('a')\n",
    "    \n",
    "    for a_tag in a_tags:\n",
    "        url = a_tag.get('href')\n",
    "        if url:\n",
    "            links_para_baixar.append(url)\n",
    "            \n",
    "    return links_para_baixar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa7c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_arquivo(url, pasta_destino, max_retries, retry_delay):\n",
    "\n",
    "    \"\"\"\n",
    "    Baixa um arquivo com retentativas. Se for .zip, extrai. Se .csv, salva.\n",
    "    RETORNA uma string de status em vez de imprimir.\n",
    "    \"\"\"\n",
    "\n",
    "    nome_arquivo = os.path.basename(url)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            \n",
    "            if nome_arquivo.endswith('.zip'):\n",
    "                response = requests.get(url) \n",
    "                response.raise_for_status()\n",
    "                \n",
    "                with zipfile.ZipFile(io.BytesIO(response.content)) as zf:\n",
    "                    zf.extractall(pasta_destino)\n",
    "                    nomes_extraidos = zf.namelist()\n",
    "                \n",
    "                return f\"[EXTRAÍDO] {nome_arquivo} -> {', '.join(nomes_extraidos)}\"\n",
    "\n",
    "            else:\n",
    "                caminho_local = os.path.join(pasta_destino, nome_arquivo)\n",
    "                \n",
    "                with requests.get(url, stream=True) as r:\n",
    "                    r.raise_for_status()\n",
    "                    with open(caminho_local, 'wb') as f:\n",
    "                        for chunk in r.iter_content(chunk_size=8192): \n",
    "                            f.write(chunk)\n",
    "                            \n",
    "                return f\"[SALVO] {nome_arquivo}\"\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt + 1 < max_retries:\n",
    "                time.sleep(retry_delay)\n",
    "            else: \n",
    "                return f\"[FALHA-REDE] {nome_arquivo} após {max_retries} tentativas. Erro: {e}\"\n",
    "        \n",
    "        except zipfile.BadZipFile:\n",
    "            return f\"[FALHA-ZIP] {nome_arquivo} está corrompido ou não é .zip.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"[FALHA-INESPERADA] {nome_arquivo}. Erro: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06d05d",
   "metadata": {},
   "source": [
    "#### **Credenciais**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a0ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Credenciais do PostgreSQL\n",
    "usuario_pg = os.getenv(\"POSTGRES_USER\")\n",
    "senha_pg = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "host_pg = os.getenv(\"POSTGRES_HOST\")\n",
    "porta_pg = os.getenv(\"POSTGRES_PORT\")\n",
    "banco_pg = os.getenv(\"POSTGRES_DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ad762",
   "metadata": {},
   "source": [
    "#### **Variaveis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número máximo de tentativas por arquivo\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "# Segundos de espera entre as tentativas\n",
    "RETRY_DELAY = 5  \n",
    "\n",
    "# Maximo de threads\n",
    "MAX_WORKERS = 20\n",
    "\n",
    "# URL da página para extrair os links\n",
    "PAGE_URL = 'https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/serie-historica-de-precos-de-combustiveis'\n",
    "\n",
    "# Pasta onde os arquivos serão baixados\n",
    "DOWNLOAD_DIR = 'arquivos_combustiveis_automotivos'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d70f28a",
   "metadata": {},
   "source": [
    "#### **Aula 1 - Processos e formas de análise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4ed4ab",
   "metadata": {},
   "source": [
    "#### **Aula 2 - Ligação com bancos de dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598075b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar os arquivos da ANP\n",
    "\n",
    "# 1. Criar a pasta de download\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "# 2. Baixar o HTML da página da ANP\n",
    "print(f\"Acessando a página: {PAGE_URL}\")\n",
    "\n",
    "try:\n",
    "    response = requests.get(PAGE_URL)\n",
    "    response.raise_for_status()\n",
    "    conteudo_html = response.text\n",
    "    \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Erro fatal ao acessar a página da ANP: {e}\")\n",
    "    print(\"Verifique sua conexão com a internet ou se a URL da ANP mudou\")\n",
    "    sys.exit(1) \n",
    "\n",
    "# 3. Analisar (parse) o HTML\n",
    "soup = BeautifulSoup(conteudo_html, 'html.parser')\n",
    "\n",
    "# 4. Encontrar os links\n",
    "links = encontrar_links(soup)\n",
    "\n",
    "if not links:\n",
    "    print(\"Nenhum link encontrado para baixar\")\n",
    "\n",
    "else:\n",
    "    print(f\"Encontrados {len(links)} arquivos para baixar na seção 'Combustíveis automotivos'\")\n",
    "    print()\n",
    "\n",
    "    # 5. Processar (baixar ou extrair) cada arquivo EM PARALELO\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        print(f\"Iniciando downloads com até {MAX_WORKERS} threads em paralelo...\")\n",
    "        print(\"-\" * 70)\n",
    "        future_to_url = {}\n",
    "\n",
    "        for url in links:\n",
    "            future = executor.submit(processar_arquivo, \n",
    "                                     url, \n",
    "                                     DOWNLOAD_DIR, \n",
    "                                     MAX_RETRIES, \n",
    "                                     RETRY_DELAY)\n",
    "            future_to_url[future] = url\n",
    "\n",
    "        num_concluidos = 0\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            num_concluidos += 1\n",
    "            url = future_to_url[future]\n",
    "            \n",
    "            try:\n",
    "                status_message = future.result()\n",
    "                print(f\"({num_concluidos}/{len(links)}) {status_message}\")\n",
    "                \n",
    "            except Exception as exc:\n",
    "                print(f\"({num_concluidos}/{len(links)}) [FALHA-GERAL] {url} gerou uma exceção: {exc}\")\n",
    "                \n",
    "    print(\"-\" * 70)\n",
    "    print()\n",
    "    print(\"Download e processamento de todos os arquivos concluído\")\n",
    "    print(f\"Os arquivos estão salvos em: {os.path.abspath(DOWNLOAD_DIR)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
